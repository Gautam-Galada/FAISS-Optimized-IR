{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain langchain_community jq\n",
    "!pip install faiss-gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import JSONLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "json_file = \"articles.json\"\n",
    "topic = \"Health\"\n",
    "\n",
    "\n",
    "loader = JSONLoader(\n",
    "    file_path=json_file,\n",
    "    jq_schema=f'.{topic}[]',\n",
    "    content_key=\"summary\"\n",
    ")\n",
    "\n",
    "documents = loader.load()\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents for topic: {topic}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Break documents into smaller chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=20)\n",
    "split_documents = []\n",
    "for doc in tqdm(documents, desc=\"Chunking documents\"):\n",
    "    chunks = text_splitter.split_text(doc.page_content)\n",
    "    split_documents.extend([\n",
    "        Document(page_content=chunk, metadata=doc.metadata) for chunk in chunks\n",
    "    ])\n",
    "\n",
    "print(f\"Generated {len(split_documents)} chunks from the documents.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-MiniLM-L6-v2\")\n",
    "def compute_embeddings_parallel(documents, embedding_model):\n",
    "    embeddings = []\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        for embedding in tqdm(\n",
    "            executor.map(embedding_model.embed_query, [doc.page_content for doc in documents]),\n",
    "            total=len(documents),\n",
    "            desc=\"Processing documents\"\n",
    "        ):\n",
    "            embeddings.append(embedding)\n",
    "    return embeddings\n",
    "\n",
    "print(\"Creating FAISS vector database for {} documents...\".format(len(split_documents)))\n",
    "embeddings = compute_embeddings_parallel(split_documents, embedding_model)\n",
    "vectorstore = FAISS.from_documents(split_documents, embedding_model)\n",
    "print(\"FAISS vector database created successfully.\")\n",
    "vectorstore.save_local(\"/content/drive/MyDrive/faiss_index\")\n",
    "print(\"FAISS index saved to disk.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FAISS index from disk (if already saved)\n",
    "vectorstore = FAISS.load_local(\"/content/drive/MyDrive/faiss_index\", embedding_model, allow_dangerous_deserialization = True)\n",
    "print(\"Loaded FAISS index from disk.\")\n",
    "# User query\n",
    "user_query = input(\"Enter your question: \")\n",
    "\n",
    "# Retrieve the most relevant context using Maximum Marginal Relevance\n",
    "retriever = vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 3, \"fetch_k\": 6})\n",
    "results = retriever.get_relevant_documents(user_query)  # Correct method\n",
    "\n",
    "# Extract context\n",
    "if results:\n",
    "    context = results[0].page_content\n",
    "    print(\"\\nRetrieved Context:\")\n",
    "    print(context)\n",
    "else:\n",
    "    print(\"\\nNo relevant context found.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "llama "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import JSONLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_name = \"llama2\"\n",
    "llama_model = Ollama(model=model_name)\n",
    "\n",
    "explicit_prompt_template = \"\"\"\n",
    "You are an AI model that provides detailed and specific answers. Answer the question based on the context below. If you cannot answer the question, reply \"I don't know\". Limit your response to 30 words.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "explicit_prompt = PromptTemplate.from_template(explicit_prompt_template)\n",
    "hardcoded_context = \"to the yellow fever vaccine is known as yellow fever vaccine associated acute neurotropic disease yel and the canadian medical association published a 2001 cmaj article entitled yellow fever\"\n",
    "hardcoded_query = \"What is yellow fever vaccine?\"\n",
    "\n",
    "input_data = {\"context\": hardcoded_context, \"question\": hardcoded_query}\n",
    "\n",
    "start_time = time.time()\n",
    "answer = llama_model.invoke(input=explicit_prompt.format(**input_data))\n",
    "end_time = time.time()\n",
    "\n",
    "time_taken = end_time - start_time\n",
    "\n",
    "print(\"\\nLlama Answer:\")\n",
    "print(answer)\n",
    "print(f\"\\nTime Taken for Llama to Answer: {time_taken:.4f} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
